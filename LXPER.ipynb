{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "LXPER.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "r1mzKsFWV2Wd",
        "keS8UmkOV_PL",
        "sLvn-GLoe30l",
        "BN-F8IJML9YI",
        "JdoNdna5dFWP"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r1mzKsFWV2Wd"
      },
      "source": [
        "# **Import**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MOsHUjgdIrIW",
        "outputId": "aff12f91-35e3-447a-8dc6-f41d6ad44f7a"
      },
      "source": [
        "! pip install git+https://github.com/huggingface/transformers.git\n",
        "! pip install git+https://github.com/huggingface/datasets.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/huggingface/transformers.git\n",
            "  Cloning https://github.com/huggingface/transformers.git to /tmp/pip-req-build-tr11hc1x\n",
            "  Running command git clone -q https://github.com/huggingface/transformers.git /tmp/pip-req-build-tr11hc1x\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers==4.2.0.dev0) (20.8)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers==4.2.0.dev0) (1.19.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==4.2.0.dev0) (2019.12.20)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 7.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers==4.2.0.dev0) (4.41.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers==4.2.0.dev0) (2.23.0)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers==4.2.0.dev0) (0.8)\n",
            "Collecting tokenizers==0.9.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/1c/e789a8b12e28be5bc1ce2156cf87cb522b379be9cadc7ad8091a4cc107c4/tokenizers-0.9.4-cp36-cp36m-manylinux2010_x86_64.whl (2.9MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9MB 16.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers==4.2.0.dev0) (3.0.12)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers==4.2.0.dev0) (2.4.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==4.2.0.dev0) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==4.2.0.dev0) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==4.2.0.dev0) (1.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==4.2.0.dev0) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==4.2.0.dev0) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==4.2.0.dev0) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==4.2.0.dev0) (2.10)\n",
            "Building wheels for collected packages: transformers\n",
            "  Building wheel for transformers (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for transformers: filename=transformers-4.2.0.dev0-cp36-none-any.whl size=1515786 sha256=9eaffd35c0e68652c195678dcf0fc732109f715e4eef513bb55613d061518f47\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-l2jaz4gx/wheels/33/eb/3b/4bf5dd835e865e472d4fc0754f35ac0edb08fe852e8f21655f\n",
            "Successfully built transformers\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893261 sha256=a40d5c62d63a4a56d8fcc670ee00c7b6f27de6db5e7ccb1792c6ed2cae67cf72\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sacremoses, tokenizers, transformers\n",
            "Successfully installed sacremoses-0.0.43 tokenizers-0.9.4 transformers-4.2.0.dev0\n",
            "Collecting git+https://github.com/huggingface/datasets.git\n",
            "  Cloning https://github.com/huggingface/datasets.git to /tmp/pip-req-build-4xnbhxxx\n",
            "  Running command git clone -q https://github.com/huggingface/datasets.git /tmp/pip-req-build-4xnbhxxx\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.6/dist-packages (from datasets==1.1.3) (1.19.4)\n",
            "Collecting pyarrow>=0.17.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/e1/27958a70848f8f7089bff8d6ebe42519daf01f976d28b481e1bfd52c8097/pyarrow-2.0.0-cp36-cp36m-manylinux2014_x86_64.whl (17.7MB)\n",
            "\u001b[K     |████████████████████████████████| 17.7MB 210kB/s \n",
            "\u001b[?25hRequirement already satisfied: dill in /usr/local/lib/python3.6/dist-packages (from datasets==1.1.3) (0.3.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from datasets==1.1.3) (1.1.5)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.6/dist-packages (from datasets==1.1.3) (2.23.0)\n",
            "Requirement already satisfied: tqdm<4.50.0,>=4.27 in /usr/local/lib/python3.6/dist-packages (from datasets==1.1.3) (4.41.1)\n",
            "Collecting xxhash\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f7/73/826b19f3594756cb1c6c23d2fbd8ca6a77a9cd3b650c9dec5acc85004c38/xxhash-2.0.0-cp36-cp36m-manylinux2010_x86_64.whl (242kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 51.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: multiprocess in /usr/local/lib/python3.6/dist-packages (from datasets==1.1.3) (0.70.11.1)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from datasets==1.1.3) (0.8)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas->datasets==1.1.3) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->datasets==1.1.3) (2018.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->datasets==1.1.3) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->datasets==1.1.3) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->datasets==1.1.3) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->datasets==1.1.3) (3.0.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.7.3->pandas->datasets==1.1.3) (1.15.0)\n",
            "Building wheels for collected packages: datasets\n",
            "  Building wheel for datasets (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for datasets: filename=datasets-1.1.3-cp36-none-any.whl size=158234 sha256=9b16bff4769c0e0938e1c55478b1b0342844657d05203aacabd4293127a483d7\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-7x5spzbm/wheels/3e/af/ff/d1cdb5d0f9cff6eba2042a16b477ada497e23f1a3b6950b928\n",
            "Successfully built datasets\n",
            "Installing collected packages: pyarrow, xxhash, datasets\n",
            "  Found existing installation: pyarrow 0.14.1\n",
            "    Uninstalling pyarrow-0.14.1:\n",
            "      Successfully uninstalled pyarrow-0.14.1\n",
            "Successfully installed datasets-1.1.3 pyarrow-2.0.0 xxhash-2.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vSSrG8aXhD4F",
        "outputId": "2c4b7ef5-101b-4db6-ce47-fd691d417d13"
      },
      "source": [
        "!git clone https://github.com/huggingface/transformers.git"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'transformers'...\n",
            "remote: Enumerating objects: 7, done.\u001b[K\n",
            "remote: Counting objects: 100% (7/7), done.\u001b[K\n",
            "remote: Compressing objects: 100% (7/7), done.\u001b[K\n",
            "remote: Total 57029 (delta 0), reused 0 (delta 0), pack-reused 57022\u001b[K\n",
            "Receiving objects: 100% (57029/57029), 42.41 MiB | 28.44 MiB/s, done.\n",
            "Resolving deltas: 100% (39995/39995), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sf-upMNvV7tn"
      },
      "source": [
        "# **Fine-Tuning**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hVXAgUhymmfn"
      },
      "source": [
        "0.687653073292803로 가장 성능이 좋았던 ELECTRA 모델을 사용합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "keS8UmkOV_PL"
      },
      "source": [
        "## ELECTRA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQ1kejmoeP1Q"
      },
      "source": [
        " + epoch: 1, batch_size: 32, learning_rate: 2e-5, mattews_correlaion: 0.6125472225786625\r\n",
        " + epoch: 2, batch_size: 32, learning_rate: 2e-5, mattews_correlaion: 0.6440774707016963\r\n",
        " + epoch: 3, batch_size: 16, learning_rate: 2e-5, mattews_correlaion: 0.687653073292803\r\n",
        " + epoch: 3, batch_size: 32, learning_rate: 2e-5, mattews_correlaion: 0.6803092970928678\r\n",
        " + epoch: 4, batch_size: 32, learning_rate: 2e-5, mattews_correlaion: 0.6711447530455341\r\n",
        " + epoch: 5, batch_size: 32, learning_rate: 2e-5, mattews_correlaion: 0.6607765164674521\r\n",
        "\r\n",
        "\r\n",
        "**=> epoch 3, batch 16, learning rate 2e-5**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a2tE5ff6hI6j",
        "outputId": "d98de5a6-3c87-48dc-c72e-04bdc369a9be"
      },
      "source": [
        "# 기존에 있던 파일 삭제\r\n",
        "!rm -r electra\r\n",
        "# 학습\r\n",
        "!python transformers/examples/text-classification/run_glue.py \\\r\n",
        "  --model_name_or_path google/electra-base-discriminator \\\r\n",
        "  --task_name cola \\\r\n",
        "  --do_train \\\r\n",
        "  --do_eval \\\r\n",
        "  --max_seq_length 128 \\\r\n",
        "  --per_device_train_batch_size 16 \\\r\n",
        "  --learning_rate 2e-5 \\\r\n",
        "  --num_train_epochs 3 \\\r\n",
        "  --output_dir electra"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rm: cannot remove 'electra': No such file or directory\n",
            "2020-12-20 05:25:32.282722: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
            "12/20/2020 05:25:34 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "12/20/2020 05:25:34 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir=electra, overwrite_output_dir=False, do_train=True, do_eval=True, do_predict=False, model_parallel=False, evaluation_strategy=EvaluationStrategy.NO, prediction_loss_only=False, per_device_train_batch_size=16, per_device_eval_batch_size=8, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=2e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=-1, warmup_steps=0, logging_dir=runs/Dec20_05-25-34_6c721fec3cb4, logging_first_step=False, logging_steps=500, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level=O1, local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name=electra, disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, fp16_backend=auto, sharded_ddp=False)\n",
            "Downloading: 28.7kB [00:00, 16.3MB/s]       \n",
            "Downloading: 28.7kB [00:00, 18.2MB/s]       \n",
            "Downloading and preparing dataset glue/cola (download: 368.14 KiB, generated: 596.73 KiB, post-processed: Unknown size, total: 964.86 KiB) to /root/.cache/huggingface/datasets/glue/cola/1.0.0/7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4...\n",
            "Downloading: 100% 377k/377k [00:00<00:00, 12.6MB/s]\n",
            "Dataset glue downloaded and prepared to /root/.cache/huggingface/datasets/glue/cola/1.0.0/7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4. Subsequent calls will reuse this data.\n",
            "12/20/2020 05:25:36 - INFO - filelock -   Lock 140481403535936 acquired on /root/.cache/huggingface/transformers/7d1569a4df2372d67341bda716bce4e3edf3e3ffadb97251bc4b6b35d459f624.2f3596f15f58505e0f3a1b5b28d8a37b25b653ee06d18272368f53df840e6323.lock\n",
            "[INFO|file_utils.py:1301] 2020-12-20 05:25:36,842 >> https://huggingface.co/google/electra-base-discriminator/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpzm1k_gvs\n",
            "Downloading: 100% 467/467 [00:00<00:00, 299kB/s]\n",
            "[INFO|file_utils.py:1305] 2020-12-20 05:25:37,048 >> storing https://huggingface.co/google/electra-base-discriminator/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/7d1569a4df2372d67341bda716bce4e3edf3e3ffadb97251bc4b6b35d459f624.2f3596f15f58505e0f3a1b5b28d8a37b25b653ee06d18272368f53df840e6323\n",
            "[INFO|file_utils.py:1308] 2020-12-20 05:25:37,048 >> creating metadata file for /root/.cache/huggingface/transformers/7d1569a4df2372d67341bda716bce4e3edf3e3ffadb97251bc4b6b35d459f624.2f3596f15f58505e0f3a1b5b28d8a37b25b653ee06d18272368f53df840e6323\n",
            "12/20/2020 05:25:37 - INFO - filelock -   Lock 140481403535936 released on /root/.cache/huggingface/transformers/7d1569a4df2372d67341bda716bce4e3edf3e3ffadb97251bc4b6b35d459f624.2f3596f15f58505e0f3a1b5b28d8a37b25b653ee06d18272368f53df840e6323.lock\n",
            "[INFO|configuration_utils.py:431] 2020-12-20 05:25:37,049 >> loading configuration file https://huggingface.co/google/electra-base-discriminator/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/7d1569a4df2372d67341bda716bce4e3edf3e3ffadb97251bc4b6b35d459f624.2f3596f15f58505e0f3a1b5b28d8a37b25b653ee06d18272368f53df840e6323\n",
            "[INFO|configuration_utils.py:467] 2020-12-20 05:25:37,050 >> Model config ElectraConfig {\n",
            "  \"architectures\": [\n",
            "    \"ElectraForPreTraining\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"embedding_size\": 768,\n",
            "  \"finetuning_task\": \"cola\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"electra\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"summary_activation\": \"gelu\",\n",
            "  \"summary_last_dropout\": 0.1,\n",
            "  \"summary_type\": \"first\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:431] 2020-12-20 05:25:37,255 >> loading configuration file https://huggingface.co/google/electra-base-discriminator/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/7d1569a4df2372d67341bda716bce4e3edf3e3ffadb97251bc4b6b35d459f624.2f3596f15f58505e0f3a1b5b28d8a37b25b653ee06d18272368f53df840e6323\n",
            "[INFO|configuration_utils.py:467] 2020-12-20 05:25:37,255 >> Model config ElectraConfig {\n",
            "  \"architectures\": [\n",
            "    \"ElectraForPreTraining\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"embedding_size\": 768,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"electra\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"summary_activation\": \"gelu\",\n",
            "  \"summary_last_dropout\": 0.1,\n",
            "  \"summary_type\": \"first\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "12/20/2020 05:25:37 - INFO - filelock -   Lock 140481131699448 acquired on /root/.cache/huggingface/transformers/fe616facc71d8e3afc69de3edac76bf1e4a0a741e80d9a99a2cc6a9a8f5f74b5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99.lock\n",
            "[INFO|file_utils.py:1301] 2020-12-20 05:25:37,466 >> https://huggingface.co/google/electra-base-discriminator/resolve/main/vocab.txt not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpdddw71p_\n",
            "Downloading: 100% 232k/232k [00:00<00:00, 897kB/s]\n",
            "[INFO|file_utils.py:1305] 2020-12-20 05:25:37,934 >> storing https://huggingface.co/google/electra-base-discriminator/resolve/main/vocab.txt in cache at /root/.cache/huggingface/transformers/fe616facc71d8e3afc69de3edac76bf1e4a0a741e80d9a99a2cc6a9a8f5f74b5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
            "[INFO|file_utils.py:1308] 2020-12-20 05:25:37,934 >> creating metadata file for /root/.cache/huggingface/transformers/fe616facc71d8e3afc69de3edac76bf1e4a0a741e80d9a99a2cc6a9a8f5f74b5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
            "12/20/2020 05:25:37 - INFO - filelock -   Lock 140481131699448 released on /root/.cache/huggingface/transformers/fe616facc71d8e3afc69de3edac76bf1e4a0a741e80d9a99a2cc6a9a8f5f74b5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99.lock\n",
            "12/20/2020 05:25:38 - INFO - filelock -   Lock 140481131698888 acquired on /root/.cache/huggingface/transformers/81840ac426bf0d690bfb69a4ec7d706e8853d8ab309e7decb6b72ab939d6682e.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4.lock\n",
            "[INFO|file_utils.py:1301] 2020-12-20 05:25:38,147 >> https://huggingface.co/google/electra-base-discriminator/resolve/main/tokenizer.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpfguqg78m\n",
            "Downloading: 100% 466k/466k [00:00<00:00, 1.47MB/s]\n",
            "[INFO|file_utils.py:1305] 2020-12-20 05:25:38,676 >> storing https://huggingface.co/google/electra-base-discriminator/resolve/main/tokenizer.json in cache at /root/.cache/huggingface/transformers/81840ac426bf0d690bfb69a4ec7d706e8853d8ab309e7decb6b72ab939d6682e.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
            "[INFO|file_utils.py:1308] 2020-12-20 05:25:38,676 >> creating metadata file for /root/.cache/huggingface/transformers/81840ac426bf0d690bfb69a4ec7d706e8853d8ab309e7decb6b72ab939d6682e.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
            "12/20/2020 05:25:38 - INFO - filelock -   Lock 140481131698888 released on /root/.cache/huggingface/transformers/81840ac426bf0d690bfb69a4ec7d706e8853d8ab309e7decb6b72ab939d6682e.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4.lock\n",
            "[INFO|tokenization_utils_base.py:1802] 2020-12-20 05:25:38,677 >> loading file https://huggingface.co/google/electra-base-discriminator/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/fe616facc71d8e3afc69de3edac76bf1e4a0a741e80d9a99a2cc6a9a8f5f74b5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
            "[INFO|tokenization_utils_base.py:1802] 2020-12-20 05:25:38,677 >> loading file https://huggingface.co/google/electra-base-discriminator/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/81840ac426bf0d690bfb69a4ec7d706e8853d8ab309e7decb6b72ab939d6682e.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
            "12/20/2020 05:25:38 - INFO - filelock -   Lock 140481403505800 acquired on /root/.cache/huggingface/transformers/aed576b8aec823c870feda40d60bd803ac8e40056ecb7d7f43dd0b2bfd82e373.db390a2059e53ead2bb00e1a2f8cd50b0a47e1969d180cd70339ec3f6f29dce1.lock\n",
            "[INFO|file_utils.py:1301] 2020-12-20 05:25:38,903 >> https://huggingface.co/google/electra-base-discriminator/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp9s080o2l\n",
            "Downloading: 100% 440M/440M [00:05<00:00, 85.0MB/s]\n",
            "[INFO|file_utils.py:1305] 2020-12-20 05:25:44,643 >> storing https://huggingface.co/google/electra-base-discriminator/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/aed576b8aec823c870feda40d60bd803ac8e40056ecb7d7f43dd0b2bfd82e373.db390a2059e53ead2bb00e1a2f8cd50b0a47e1969d180cd70339ec3f6f29dce1\n",
            "[INFO|file_utils.py:1308] 2020-12-20 05:25:44,643 >> creating metadata file for /root/.cache/huggingface/transformers/aed576b8aec823c870feda40d60bd803ac8e40056ecb7d7f43dd0b2bfd82e373.db390a2059e53ead2bb00e1a2f8cd50b0a47e1969d180cd70339ec3f6f29dce1\n",
            "12/20/2020 05:25:44 - INFO - filelock -   Lock 140481403505800 released on /root/.cache/huggingface/transformers/aed576b8aec823c870feda40d60bd803ac8e40056ecb7d7f43dd0b2bfd82e373.db390a2059e53ead2bb00e1a2f8cd50b0a47e1969d180cd70339ec3f6f29dce1.lock\n",
            "[INFO|modeling_utils.py:1024] 2020-12-20 05:25:44,643 >> loading weights file https://huggingface.co/google/electra-base-discriminator/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/aed576b8aec823c870feda40d60bd803ac8e40056ecb7d7f43dd0b2bfd82e373.db390a2059e53ead2bb00e1a2f8cd50b0a47e1969d180cd70339ec3f6f29dce1\n",
            "[WARNING|modeling_utils.py:1132] 2020-12-20 05:25:48,465 >> Some weights of the model checkpoint at google/electra-base-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense_prediction.bias']\n",
            "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_utils.py:1143] 2020-12-20 05:25:48,465 >> Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-base-discriminator and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "100% 9/9 [00:00<00:00, 12.62ba/s]\n",
            "100% 2/2 [00:00<00:00, 26.74ba/s]\n",
            "100% 2/2 [00:00<00:00, 24.68ba/s]\n",
            "12/20/2020 05:25:49 - INFO - __main__ -   Sample 1824 of the training set: {'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'idx': 1824, 'input_ids': [101, 1045, 8969, 2008, 2026, 2269, 1010, 2002, 2001, 4389, 2004, 2019, 13547, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'label': 0, 'sentence': 'I acknowledged that my father, he was tight as an owl.', 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "12/20/2020 05:25:49 - INFO - __main__ -   Sample 409 of the training set: {'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'idx': 409, 'input_ids': [101, 2005, 2032, 2000, 2079, 2008, 2052, 2022, 1037, 6707, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'label': 1, 'sentence': 'For him to do that would be a mistake.', 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "12/20/2020 05:25:49 - INFO - __main__ -   Sample 4506 of the training set: {'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'idx': 4506, 'input_ids': [101, 2984, 6369, 1037, 2299, 1010, 2021, 3389, 2196, 2106, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'label': 1, 'sentence': 'Mary sang a song, but Lee never did.', 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "Downloading: 4.39kB [00:00, 3.16MB/s]       \n",
            "[INFO|trainer.py:388] 2020-12-20 05:26:04,543 >> The following columns in the training set don't have a corresponding argument in `ElectraForSequenceClassification.forward` and have been ignored: idx, sentence.\n",
            "[INFO|trainer.py:388] 2020-12-20 05:26:04,544 >> The following columns in the evaluation set don't have a corresponding argument in `ElectraForSequenceClassification.forward` and have been ignored: idx, sentence.\n",
            "[INFO|trainer.py:703] 2020-12-20 05:26:04,548 >> ***** Running training *****\n",
            "[INFO|trainer.py:704] 2020-12-20 05:26:04,548 >>   Num examples = 8551\n",
            "[INFO|trainer.py:705] 2020-12-20 05:26:04,548 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:706] 2020-12-20 05:26:04,549 >>   Instantaneous batch size per device = 16\n",
            "[INFO|trainer.py:707] 2020-12-20 05:26:04,549 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "[INFO|trainer.py:708] 2020-12-20 05:26:04,549 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:709] 2020-12-20 05:26:04,549 >>   Total optimization steps = 1605\n",
            "{'loss': 0.39991650390625, 'learning_rate': 1.3769470404984425e-05, 'epoch': 0.9345794392523364}\n",
            " 31% 500/1605 [02:54<06:41,  2.75it/s][INFO|trainer.py:1229] 2020-12-20 05:28:59,110 >> Saving model checkpoint to electra/checkpoint-500\n",
            "[INFO|configuration_utils.py:289] 2020-12-20 05:28:59,112 >> Configuration saved in electra/checkpoint-500/config.json\n",
            "[INFO|modeling_utils.py:814] 2020-12-20 05:29:00,745 >> Model weights saved in electra/checkpoint-500/pytorch_model.bin\n",
            "{'loss': 0.2421263885498047, 'learning_rate': 7.538940809968847e-06, 'epoch': 1.8691588785046729}\n",
            " 62% 1000/1605 [06:00<03:41,  2.73it/s][INFO|trainer.py:1229] 2020-12-20 05:32:05,310 >> Saving model checkpoint to electra/checkpoint-1000\n",
            "[INFO|configuration_utils.py:289] 2020-12-20 05:32:05,312 >> Configuration saved in electra/checkpoint-1000/config.json\n",
            "[INFO|modeling_utils.py:814] 2020-12-20 05:32:06,886 >> Model weights saved in electra/checkpoint-1000/pytorch_model.bin\n",
            "{'loss': 0.1498493194580078, 'learning_rate': 1.308411214953271e-06, 'epoch': 2.803738317757009}\n",
            " 93% 1500/1605 [09:07<00:37,  2.77it/s][INFO|trainer.py:1229] 2020-12-20 05:35:11,875 >> Saving model checkpoint to electra/checkpoint-1500\n",
            "[INFO|configuration_utils.py:289] 2020-12-20 05:35:11,877 >> Configuration saved in electra/checkpoint-1500/config.json\n",
            "[INFO|modeling_utils.py:814] 2020-12-20 05:35:13,553 >> Model weights saved in electra/checkpoint-1500/pytorch_model.bin\n",
            "100% 1605/1605 [09:50<00:00,  3.21it/s][INFO|trainer.py:863] 2020-12-20 05:35:55,179 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 590.6301, 'train_samples_per_second': 2.717, 'epoch': 3.0}\n",
            "100% 1605/1605 [09:50<00:00,  2.72it/s]\n",
            "[INFO|trainer.py:1229] 2020-12-20 05:35:55,194 >> Saving model checkpoint to electra\n",
            "[INFO|configuration_utils.py:289] 2020-12-20 05:35:55,196 >> Configuration saved in electra/config.json\n",
            "[INFO|modeling_utils.py:814] 2020-12-20 05:35:56,798 >> Model weights saved in electra/pytorch_model.bin\n",
            "12/20/2020 05:35:56 - INFO - __main__ -   ***** Train results *****\n",
            "12/20/2020 05:35:56 - INFO - __main__ -     epoch = 3.0\n",
            "12/20/2020 05:35:56 - INFO - __main__ -     train_runtime = 590.6301\n",
            "12/20/2020 05:35:56 - INFO - __main__ -     train_samples_per_second = 2.717\n",
            "12/20/2020 05:35:56 - INFO - __main__ -   *** Evaluate ***\n",
            "[INFO|trainer.py:388] 2020-12-20 05:35:56,838 >> The following columns in the evaluation set don't have a corresponding argument in `ElectraForSequenceClassification.forward` and have been ignored: idx, sentence.\n",
            "[INFO|trainer.py:1421] 2020-12-20 05:35:56,838 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:1422] 2020-12-20 05:35:56,838 >>   Num examples = 1043\n",
            "[INFO|trainer.py:1423] 2020-12-20 05:35:56,838 >>   Batch size = 8\n",
            " 99% 130/131 [00:07<00:00, 16.43it/s]12/20/2020 05:36:04 - INFO - /usr/local/lib/python3.6/dist-packages/datasets/metric.py -   Removing /root/.cache/huggingface/metrics/glue/cola/default_experiment-1-0.arrow\n",
            "100% 131/131 [00:08<00:00, 16.23it/s]\n",
            "12/20/2020 05:36:04 - INFO - __main__ -   ***** Eval results cola *****\n",
            "12/20/2020 05:36:04 - INFO - __main__ -     epoch = 3.0\n",
            "12/20/2020 05:36:04 - INFO - __main__ -     eval_loss = 0.5178582668304443\n",
            "12/20/2020 05:36:04 - INFO - __main__ -     eval_matthews_correlation = 0.687653073292803\n",
            "12/20/2020 05:36:04 - INFO - __main__ -     eval_runtime = 8.1191\n",
            "12/20/2020 05:36:04 - INFO - __main__ -     eval_samples_per_second = 128.462\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLvn-GLoe30l"
      },
      "source": [
        "## ALBERT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q1GDPyXQe307"
      },
      "source": [
        " + epoch: 1, batch_size: 32, learning_rate: 2e-5, mattews_correlaion: 0.4675029019890418\r\n",
        " + epoch: 3, batch_size: 32, learning_rate: 2e-5, mattews_correlaion: 0.5526213553961862\r\n",
        " + epoch: 4, batch_size: 32, learning_rate: 2e-5, mattews_correlaion: 0.5336284672741207"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EISRskNSe308"
      },
      "source": [
        "# 기존에 있던 파일 삭제\r\n",
        "!rm -r albert\r\n",
        "# 학습\r\n",
        "!python transformers/examples/text-classification/run_glue.py \\\r\n",
        "  --model_name_or_path albert-base-v2 \\\r\n",
        "  --task_name cola \\\r\n",
        "  --do_train \\\r\n",
        "  --do_eval \\\r\n",
        "  --max_seq_length 128 \\\r\n",
        "  --per_device_train_batch_size 32 \\\r\n",
        "  --learning_rate 2e-5 \\\r\n",
        "  --num_train_epochs 4 \\\r\n",
        "  --output_dir albert"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BN-F8IJML9YI"
      },
      "source": [
        "## DISTILBERT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AKGtQPIJL9Yi"
      },
      "source": [
        " + epoch: 3, batch_size: 32, learning_rate: 2e-5, mattews_correlaion: 0.5032312800977334\r\n",
        " + epoch: 5, batch_size: 32, learning_rate: 2e-5, mattews_correlaion: 0.5155709926752544\r\n",
        " + epoch: 6, batch_size: 32, learning_rate: 2e-5, mattews_correlaion: 0.5489250601752835\r\n",
        " + epoch: 6, batch_size: 32, learning_rate: 2e-5, mattews_correlaion: 0.5549749054428225"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K3Jb_mqmL9Yl"
      },
      "source": [
        "# 기존에 있던 파일 삭제\r\n",
        "!rm -r distilbert\r\n",
        "# 학습\r\n",
        "!python transformers/examples/text-classification/run_glue.py \\\r\n",
        "  --model_name_or_path distilbert-base-uncased \\\r\n",
        "  --task_name cola \\\r\n",
        "  --do_train \\\r\n",
        "  --do_eval \\\r\n",
        "  --max_seq_length 128 \\\r\n",
        "  --per_device_train_batch_size 32 \\\r\n",
        "  --learning_rate 2e-5 \\\r\n",
        "  --num_train_epochs 7 \\\r\n",
        "  --output_dir distilbert"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JdoNdna5dFWP"
      },
      "source": [
        "## ROBERTA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iQscYqlSdFWa"
      },
      "source": [
        " + epoch: 1, batch_size: 32, learning_rate: 2e-5, mattews_correlaion: 0.5406003416615092\r\n",
        " + epoch: 3, batch_size: 32, learning_rate: 2e-5, mattews_correlaion: 0.5856531698367675\r\n",
        " + epoch: 5, batch_size: 32, learning_rate: 2e-5, mattews_correlaion: 0.6031465729998459\r\n",
        " + epoch: 7, batch_size: 32, learning_rate: 2e-5, mattews_correlaion: 0.5933815828411364"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rIWiwLspdFWb"
      },
      "source": [
        "# 기존에 있던 파일 삭제\r\n",
        "!rm -r roberta\r\n",
        "# 학습\r\n",
        "!python transformers/examples/text-classification/run_glue.py \\\r\n",
        "  --model_name_or_path roberta-base \\\r\n",
        "  --task_name cola \\\r\n",
        "  --do_train \\\r\n",
        "  --do_eval \\\r\n",
        "  --max_seq_length 128 \\\r\n",
        "  --per_device_train_batch_size 32 \\\r\n",
        "  --learning_rate 2e-5 \\\r\n",
        "  --num_train_epochs 6 \\\r\n",
        "  --output_dir roberta"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eF-TFRXHi0nu"
      },
      "source": [
        "## XLNET"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X2dZswegi0oQ"
      },
      "source": [
        " + epoch: 1, batch_size: 32, learning_rate: 2e-5, mattews_correlaion: 0.20235584969980377\r\n",
        " + epoch: 3, batch_size: 32, learning_rate: 2e-5, mattews_correlaion: 0.315911143820797\r\n",
        " + epoch: 3, batch_size: 32, learning_rate: 3e-5, mattews_correlaion: 0.4257622461369574\r\n",
        " + epoch: 5, batch_size: 32, learning_rate: 5e-5, mattews_correlaion: 0.4297611529956954\r\n",
        " + epoch: 5, batch_size: 32, learning_rate: 3e-5, mattews_correlaion: 0.4481341938330174"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "df-hZ1hSi0od"
      },
      "source": [
        "# 기존에 있던 파일 삭제\r\n",
        "!rm -r xlnet\r\n",
        "# 학습\r\n",
        "!python transformers/examples/text-classification/run_glue.py \\\r\n",
        "  --model_name_or_path xlnet-base-cased \\\r\n",
        "  --task_name cola \\\r\n",
        "  --do_train \\\r\n",
        "  --do_eval \\\r\n",
        "  --max_seq_length 128 \\\r\n",
        "  --per_device_train_batch_size 32 \\\r\n",
        "  --learning_rate 3e-5 \\\r\n",
        "  --num_train_epochs 5 \\\r\n",
        "  --output_dir xlnet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_IpbIyIR8a4K"
      },
      "source": [
        "# **Test**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Iz6_BZd8h_H",
        "outputId": "6e863e57-6d07-49c2-aa99-4ba6c1fcf739"
      },
      "source": [
        "%cd electra\r\n",
        "!zip -9 /content/model.zip *.bin *.json vocab.txt\r\n",
        "%cd .."
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/electra\n",
            "  adding: pytorch_model.bin (deflated 7%)\n",
            "  adding: training_args.bin (deflated 44%)\n",
            "  adding: config.json (deflated 52%)\n",
            "  adding: special_tokens_map.json (deflated 40%)\n",
            "  adding: tokenizer_config.json (deflated 35%)\n",
            "  adding: trainer_state.json (deflated 56%)\n",
            "  adding: vocab.txt (deflated 53%)\n",
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fF2HNib9PJPS"
      },
      "source": [
        "import os\r\n",
        "import zipfile\r\n",
        "\r\n",
        "import torch\r\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\r\n",
        "\r\n",
        "\r\n",
        "# transformers\r\n",
        "model_name = \"model\"\r\n",
        "\r\n",
        "if not os.path.isdir(model_name):\r\n",
        "    os.mkdir(model_name)\r\n",
        "    zipfile.ZipFile(\"model.zip\", \"r\").extractall(model_name)\r\n",
        "\r\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\r\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name)\r\n",
        "\r\n",
        "# torch\r\n",
        "is_cuda = torch.cuda.is_available()\r\n",
        "softmax = torch.nn.Softmax(dim=1)\r\n",
        "\r\n",
        "if is_cuda:\r\n",
        "    model.cuda()"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lcvlKQNxWNNk",
        "outputId": "3a2abcbc-2186-4fea-930e-42696af451c9"
      },
      "source": [
        "data = tokenizer(\"hello, my name are SeungWoo Nam\", return_tensors='pt')\r\n",
        "input_ids, token_type_ids, attention_mask = data.values()\r\n",
        "if is_cuda:\r\n",
        "    input_ids = input_ids.cuda()\r\n",
        "    token_type_ids = token_type_ids.cuda()\r\n",
        "    attention_mask = attention_mask.cuda()\r\n",
        "\r\n",
        "float(softmax(model(input_ids, token_type_ids, attention_mask)[0])[0, 1])"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.01323075033724308"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    }
  ]
}